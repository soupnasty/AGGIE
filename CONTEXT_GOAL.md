# AGGIE Memory System — Design Document

## Overview

AGGIE needs a memory system that allows natural, multi-turn conversations without arbitrary limits. Context should fade naturally based on time, not turn count.

> **Scope:** This document covers **v1 — Session Context only**. Persistent memory (explicit "remember that..." storage) is deferred to a future version.

---

## Goals

1. **Conversational continuity** — Users can have extended back-and-forth conversations without losing context mid-session.

2. **Natural session boundaries** — Context fades based on time, not turn count. Walking away for 30 minutes feels like a fresh start; rapid-fire conversation stays intact.

3. **Privacy-first** — All memory storage and processing happens locally. No conversation history leaves the device.

---

## Requirements

### Session Context

| Requirement | Description |
|-------------|-------------|
| Time-based decay | Context expires based on silence duration, not message count |
| Configurable soft decay | After N minutes of silence, older context is summarized |
| Configurable hard decay | After M minutes of silence, context is cleared entirely |
| Token management | When session exceeds a token threshold, oldest turns are summarized to stay within limits |
| Local summarization | Summaries are generated by a local model — no external API calls |
| Silent operation | Decay and summarization happen transparently without notifying the user |

### Configuration

All session behavior should be configurable:

| Setting | Purpose |
|---------|---------|
| `soft_decay_minutes` | Silence threshold before summarization (default: 10) |
| `hard_decay_minutes` | Silence threshold before clearing context (default: 30) |
| `max_session_tokens` | Token ceiling before forced summarization (default: 8000) |
| `summarizer_model` | Local summarization model (default: `facebook/bart-large-cnn`) |

---

## Architecture

### Session Context Manager

```
┌─────────────────────────────────────────────────────────────────┐
│                      SessionContext                             │
│                                                                 │
│  - Conversation turns (role, content, timestamp)                │
│  - Decay timer (tracks silence duration)                        │
│  - Token counter (estimates current context size)               │
│  - Local summarizer (lazy-loaded, runs on CPU)                  │
│                                                                 │
│  Output: build_messages() → List[Message]                       │
└─────────────────────────────────────────────────────────────────┘
```

**SessionContext** — Manages the current conversation. Holds all turns with timestamps, handles decay logic, and triggers summarization when needed. Provides messages formatted for the Claude API.

---

## Session Context Lifecycle

```
ACTIVE ──[silence > soft_decay]──▶ SUMMARIZED ──[silence > hard_decay]──▶ CLEARED
   │
   └──[tokens > max_session_tokens]──▶ SUMMARIZE OLDEST (remain ACTIVE)
```

- **Active**: Full verbatim conversation history
- **Summarized**: Older turns compressed into a summary, recent turns verbatim
- **Cleared**: Empty, fresh start

---

## Integration Points

### Daemon

- Calls `SessionContext.add_turn()` after each user/assistant exchange
- Calls `SessionContext.check_decay()` before each interaction to handle soft/hard decay
- Passes `SessionContext.build_messages()` to the LLM client

### Claude API Client

- Receives message list from `SessionContext.build_messages()`
- No changes to system prompt needed for v1

### CLI (aggie-ctl)

- `aggie-ctl context` — Show current session state (turn count, token estimate, time since last interaction)
- `aggie-ctl context --clear` — Manually clear session context

---

## Implementation Notes

### Summarization Model

- Runs on **CPU only** — GPU is reserved for faster-whisper
- Acceptable latency: 5-10 seconds (not in hot path)
- Candidate models: `facebook/bart-large-cnn` (400M), `google/flan-t5-base` (250M)
- Consider dialogue-specific models (e.g., `knkarthick/MEETING_SUMMARY`) for better "who said what" preservation

### Token Estimation

- Claude's tokenizer is not publicly available
- Use rough estimate: ~4 characters per token, or count words × 1.3
- Err on the side of summarizing earlier rather than hitting API limits

### Summarization Strategy

When summarizing, preserve:
- Key facts and decisions mentioned
- User preferences expressed
- Questions that were asked but not fully resolved

Discard:
- Pleasantries and filler
- Redundant restatements
- Superseded information (e.g., if user corrected themselves)

---

## Future Work (v2+)

The following features are **out of scope for v1** but planned for future versions:

- **Persistent memory** — Explicit "remember that..." storage with local YAML/SQLite
- **Conflict detection** — Semantic similarity to detect contradictory facts
- **Intent detection** — Local classification of memory-related commands
- **Memory injection** — Injecting relevant stored facts into system prompt
- **CLI memory management** — `aggie-ctl remember`, `aggie-ctl forget`, `aggie-ctl memories`
